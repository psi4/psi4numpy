{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Manipulation: Psi4 and NumPy manipulation routines\n",
    "Contracting tensors together forms the core of the Psi4NumPy project. First let us consider the popluar [Einstein Summation Notation](https://en.wikipedia.org/wiki/Einstein_notation) which allows for very succinct descriptions of a given tensor contraction.\n",
    "\n",
    "For example, let us consider a [inner (dot) product](https://en.wikipedia.org/wiki/Dot_product):\n",
    "$$c = \\sum_{ij} A_{ij} * B_{ij}$$\n",
    "\n",
    "With the Einsten convention all indices that are repeated are considered summed over and the explicit summation symbol is dropped:\n",
    "$$c = A_{ij} * B_{ij}$$\n",
    "\n",
    "This can be extended to [matrix multiplication](https://en.wikipedia.org/wiki/Matrix_multiplication):\n",
    "\\begin{align}\n",
    "\\rm{Conventional}\\;\\;\\;  C_{ik} &= \\sum_{j} A_{ij} * B_{jk} \\\\\n",
    "\\rm{Einstein}\\;\\;\\;  C &= A_{ij} * B_{jk} \\\\\n",
    "\\end{align}\n",
    "\n",
    "Where the $C$ matrix has *implied* indices of $C_{ik}$ as the only repeated index is $j$.\n",
    "\n",
    "However, there are many cases where this notation fails. Thus we often use the generalized Einstein convention. To demonstrate let us examine a [Hadamard product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)):\n",
    "$$C_{ij} = \\sum_{ij} A_{ij} * B_{ij}$$\n",
    "\n",
    "\n",
    "This operation is nearly identical to the dot product above, and is not able to be written in pure Einstein convention. The generalized convention allows for the use of indices on the left hand side of the equation:\n",
    "$$C_{ij} = A_{ij} * B_{ij}$$\n",
    "\n",
    "Usually it should be apparent within the context the exact meaning of a given expression.\n",
    "\n",
    "Finally we also make use of Matrix notation:\n",
    "\\begin{align}\n",
    "{\\rm Matrix}\\;\\;\\;  \\bf{D} &= \\bf{A B C} \\\\\n",
    "{\\rm Einstein}\\;\\;\\;  D_{il} &= A_{ij} B_{jk} C_{kl}\n",
    "\\end{align}\n",
    "\n",
    "Note that this notation is signified by the use of bold characters to denote matrices and consecutive matrices next to each other imply a chain of matrix multiplications! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einsum\n",
    "\n",
    "To perform most operations we turn to [NumPy's einsum function](https://docs.scipy.org/doc/numpy/reference/generated/numpy.einsum.html) which allows the Einsten convention as an input. In addition to being much easier to read, manipulate, and change it is also much more efficient that a pure Python implementation.\n",
    "\n",
    "To begin let us consider the construction of the following tensor (which you may recognize):\n",
    "$$G_{pq} = 2.0 * I_{pqrs} D_{rs} - 1.0 * I_{prqs} D_{rs}$$ \n",
    "\n",
    "First let us import our normal suite of modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import psi4\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use conventional Python loops and einsum to perform the same task using. Keep size relatively small as these I 4-index tensors grow very quickly in size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loop and einsum fock builds match:    True\n",
      "\n",
      "Time for loop G build:           0.2845 seconds\n",
      "Time for einsum G build:         0.0005 seconds\n",
      "G builds with einsum are 536.7238 times faster than Python loops!\n"
     ]
    }
   ],
   "source": [
    "size = 20\n",
    "\n",
    "if size > 30:\n",
    "    raise Exception(\"Size must be smaller than 30.\")\n",
    "D = np.random.rand(size, size)\n",
    "I = np.random.rand(size, size, size, size)\n",
    "\n",
    "# Build the fock matrix using loops, while keeping track of time\n",
    "tstart_loop = time.time()\n",
    "Gloop = np.zeros((size, size))\n",
    "for p in range(size):\n",
    "    for q in range(size):\n",
    "        for r in range(size):\n",
    "            for s in range(size):\n",
    "                Gloop[p, q] += 2 * I[p, q, r, s] * D[r, s]\n",
    "                Gloop[p, q] -=     I[p, r, q, s] * D[r, s]\n",
    "\n",
    "g_loop_time = time.time() - tstart_loop\n",
    "\n",
    "# Build the fock matrix using einsum, while keeping track of time\n",
    "tstart_einsum = time.time()\n",
    "J = np.einsum('pqrs,rs', I, D)\n",
    "K = np.einsum('prqs,rs', I, D)\n",
    "G = 2 * J - K\n",
    "\n",
    "einsum_time = time.time() - tstart_einsum\n",
    "\n",
    "# Make sure the correct answer is obtained\n",
    "print('The loop and einsum fock builds match:    %s\\n' % np.allclose(G, Gloop))\n",
    "# Print out relative times for explicit loop vs einsum Fock builds\n",
    "print('Time for loop G build:   %14.4f seconds' % g_loop_time)\n",
    "print('Time for einsum G build: %14.4f seconds' % einsum_time)\n",
    "print('G builds with einsum are {:3.4f} times faster than Python loops!'.format(g_loop_time / einsum_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the einsum function is considerbly faster than the pure Python loops and, in this authors opinion, much cleaner and easier to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dot\n",
    "\n",
    "Now let us turn our attention to a more canonical matrix multiplication example such as:\n",
    "$$D_{il} = A_{ij} B_{jk} C_{kl}$$\n",
    "\n",
    "we could perform this operation using einsum; however, matrix multiplication is an extremely common operation in all branches of linear algebra. Thus, these functions have been optimized to be more efficient than the `einsum` function. The matrix product will explicitly compute the following operation:\n",
    "$$C_{ij} = A_{ij} * B_{ij}$$\n",
    "\n",
    "This can be called with [NumPy's dot function](https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html#numpy.dot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair product allclose: True\n"
     ]
    }
   ],
   "source": [
    "size = 200\n",
    "A = np.random.rand(size, size)\n",
    "B = np.random.rand(size, size)\n",
    "C = np.random.rand(size, size)\n",
    "\n",
    "# First compute the pair product\n",
    "tmp_dot = np.dot(A, B)\n",
    "tmp_einsum = np.einsum('ij,jk->ik', A, B)\n",
    "print(\"Pair product allclose: %s\" % np.allclose(tmp_dot, tmp_einsum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have proved exactly what the dot product does, let us consider the full chain and do a timing comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain multiplication allclose: True\n",
      "\n",
      "np.dot time:\n",
      "1000 loops, best of 3: 288 Âµs per loop\n",
      "\n",
      "np.einsum time\n",
      "1 loop, best of 3: 1.68 s per loop\n"
     ]
    }
   ],
   "source": [
    "D_dot = np.dot(A, B).dot(C)\n",
    "D_einsum = np.einsum('ij,jk,kl->il', A, B, C)\n",
    "print(\"Chain multiplication allclose: %s\" % np.allclose(D_dot, D_einsum))\n",
    "\n",
    "print(\"\\nnp.dot time:\")\n",
    "%timeit np.dot(A, B).dot(C)\n",
    "\n",
    "print(\"\\nnp.einsum time\")\n",
    "%timeit np.einsum('ij,jk,kl->il', A, B, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On most machines the `np.dot` times are roughly ~3,000 times faster. The reason is twofold:\n",
    " - The `np.dot` routines typically call [Basic Linear Algebra Subprograms (BLAS)](https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms). The BLAS routines are highly optimized and threaded versions of the code.\n",
    " - The `np.einsum` code will not factorize the code so that the overall cost is ${\\cal O}(N^4)$ (as there are four indices) rather than the factored $(\\bf{A B}) \\bf{C}$ which runs ${\\cal O}(N^3)$.\n",
    " \n",
    "The first issue is difficult to overcome; however, the second issue can be resolved by the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.einsum factorized time:\n",
      "100 loops, best of 3: 5.24 ms per loop\n"
     ]
    }
   ],
   "source": [
    "print(\"np.einsum factorized time:\")\n",
    "%timeit np.einsum('ik,kl->il', np.einsum('ij,jk->ik', A, B), C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On most machines the factorized `einsum` expression is only ~20 times slower than `np.dot` while a massive improvemnt, a clear demonstration the BLAS usage is usually recommended. It is a tradeoff between speed and readability. The Psi4NumPy project tends to lean toward `einsum` usage except in case where the benefit is too large to pass up.\n",
    "\n",
    "It should be noted that in NumPy 1.12 the [einsum function](https://docs.scipy.org/doc/numpy/reference/generated/numpy.einsum.html) has a `optimize` flag which will automatically factorize the einsum code for you. However, NumPy 1.12 was recently released and is not in most installations yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex tensor manipulations\n",
    "Let us consider a popular index transformation example:\n",
    "$$M_{pqrs} = C_{pi} C_{qj} I_{ijkl} C_{rk} C_{sl}$$\n",
    "\n",
    "Here, a naive `einsum` call would scale like $\\mathcal{O}N^8$ which translations to an extremely costly computation for all but the smallest $N$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Numpy's N^8 transformation...\n",
      "...transformation complete in 22.304 seconds.\n",
      "\n",
      "Starting Numpy's N^5 transformation with einsum...\n",
      "...transformation complete in 0.004 seconds.\n",
      "\n",
      "N^5 5277.95 faster than N^8 algorithm!\n",
      "Allclose: True\n",
      "\n",
      "\n",
      "Starting Numpy's N^5 transformation with dot...\n",
      "...transformation complete in 0.001 seconds.\n",
      "\n",
      "Allclose: True\n",
      "N^5 22712.25 faster than N^8 algorithm!\n"
     ]
    }
   ],
   "source": [
    "# Grab orbitals\n",
    "size = 15\n",
    "if size > 15:\n",
    "    raise Exception(\"Size must be smaller than 15.\")\n",
    "    \n",
    "C = np.random.rand(size, size)\n",
    "I = np.random.rand(size, size, size, size)\n",
    "\n",
    "# Numpy einsum N^8 transformation.\n",
    "print(\"\\nStarting Numpy's N^8 transformation...\")\n",
    "n8_tstart = time.time()\n",
    "MO_n8 = np.einsum('pI,qJ,pqrs,rK,sL->IJKL', C, C, I, C, C)\n",
    "n8_time = time.time() - n8_tstart\n",
    "print(\"...transformation complete in %.3f seconds.\" % (n8_time))\n",
    "\n",
    "# Numpy einsum N^5 transformation.\n",
    "print(\"\\n\\nStarting Numpy's N^5 transformation with einsum...\")\n",
    "n5_tstart = time.time()\n",
    "MO_n5 = np.einsum('pA,pqrs->Aqrs', C, I)\n",
    "MO_n5 = np.einsum('qB,Aqrs->ABrs', C, MO_n5)\n",
    "MO_n5 = np.einsum('rC,ABrs->ABCs', C, MO_n5)\n",
    "MO_n5 = np.einsum('sD,ABCs->ABCD', C, MO_n5)\n",
    "n5_time = time.time() - n5_tstart\n",
    "print(\"...transformation complete in %.3f seconds.\" % n5_time)\n",
    "print(\"\\nN^5 %4.2f faster than N^8 algorithm!\" % (n8_time / n5_time))\n",
    "print(\"Allclose: %s\" % np.allclose(MO_n8, MO_n5))\n",
    "\n",
    "# Numpy GEMM N^5 transformation.\n",
    "# Try to figure this one out!\n",
    "print(\"\\n\\nStarting Numpy's N^5 transformation with dot...\")\n",
    "dgemm_tstart = time.time()\n",
    "MO = np.dot(C.T, I.reshape(size, -1))\n",
    "MO = np.dot(MO.reshape(-1, size), C)\n",
    "MO = MO.reshape(size, size, size, size).transpose(1, 0, 3, 2)\n",
    "\n",
    "MO = np.dot(C.T, MO.reshape(size, -1))\n",
    "MO = np.dot(MO.reshape(-1, size), C)\n",
    "MO = MO.reshape(size, size, size, size).transpose(1, 0, 3, 2)\n",
    "dgemm_time = time.time() - dgemm_tstart\n",
    "print(\"...transformation complete in %.3f seconds.\" % dgemm_time)\n",
    "print(\"\\nAllclose: %s\" % np.allclose(MO_n8, MO))\n",
    "print(\"N^5 %4.2f faster than N^8 algorithm!\" % (n8_time / dgemm_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
